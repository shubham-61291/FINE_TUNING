# -*- coding: utf-8 -*-
"""Last-layer Fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGGU7jO06d1ty0axl0qKe2_U49LyKMaw
"""

!pip -q install transformers datasets evaluate accelerate

import torch, matplotlib.pyplot as plt, os
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, TrainingArguments, Trainer, set_seed
)
from datasets import load_dataset
import evaluate

# Disable Weights & Biases prompts
os.environ["WANDB_DISABLED"] = "true"

set_seed(42)

# Faster demo on Colab
SMALL_RUN = True
TITLE_SUFFIX = " (SMALL RUN)" if SMALL_RUN else ""

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
if device == "cuda":
    print("GPU:", torch.cuda.get_device_name(0))

dataset = load_dataset("imdb")
if SMALL_RUN:
    dataset["train"] = dataset["train"].shuffle(seed=42).select(range(4000))
    dataset["test"]  = dataset["test"].shuffle(seed=42).select(range(2000))
dataset

model_name = "distilbert-base-uncased"   # you can swap to "bert-base-uncased" or "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

def tok_fn(batch):
    return tokenizer(batch["text"], truncation=True)

tokenized = dataset.map(tok_fn, batched=True, remove_columns=["text"])
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
tokenized

num_labels = 2
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)

# ---- 1) Freeze EVERYTHING
for p in model.parameters():
    p.requires_grad = False

# ---- 2) Unfreeze the classification head (covers DistilBERT/BERT/Roberta variants)
head_names = []
for name in ["classifier", "pre_classifier", "score"]:
    if hasattr(model, name):
        for p in getattr(model, name).parameters():
            p.requires_grad = True
        head_names.append(name)

# ---- 3) Unfreeze ONLY the last Transformer block (family-safe)
def unfreeze_last_block(m):
    # DistilBERT
    if hasattr(m, "distilbert") and hasattr(m.distilbert, "transformer"):
        for p in m.distilbert.transformer.layer[-1].parameters():
            p.requires_grad = True
        return "distilbert.transformer.layer[-1]"
    # BERT
    if hasattr(m, "bert") and hasattr(m.bert, "encoder"):
        for p in m.bert.encoder.layer[-1].parameters():
            p.requires_grad = True
        return "bert.encoder.layer[-1]"
    # RoBERTa / XLM-R (same encoder API)
    if hasattr(m, "roberta") and hasattr(m.roberta, "encoder"):
        for p in m.roberta.encoder.layer[-1].parameters():
            p.requires_grad = True
        return "roberta.encoder.layer[-1]"
    # Fallback via base_model (covers many Auto* models)
    if hasattr(m, "base_model"):
        base = m.base_model
        if hasattr(base, "encoder") and hasattr(base.encoder, "layer"):
            for p in base.encoder.layer[-1].parameters():
                p.requires_grad = True
            return "base_model.encoder.layer[-1]"
        if hasattr(base, "transformer") and hasattr(base.transformer, "layer"):
            for p in base.transformer.layer[-1].parameters():
                p.requires_grad = True
            return "base_model.transformer.layer[-1]"
    raise ValueError("Could not locate last Transformer block for this model.")

last_block_path = unfreeze_last_block(model)

# ---- Sanity: list trainable params
trainable = [n for n,p in model.named_parameters() if p.requires_grad]
print("Trainable modules:", trainable[:8], " ... (total:", len(trainable), "tensors)")
print("Head modules unfrozen:", head_names, "| Last block:", last_block_path)

# Count params
total = sum(p.numel() for p in model.parameters())
trainable_n = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total params: {total:,} | Trainable (head + last block): {trainable_n:,}")

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }

args = TrainingArguments(
    output_dir="outputs/imdb_last_layer",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,          # smaller than head-only
    weight_decay=0.01,
    logging_steps=50,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

baseline_metrics = trainer.evaluate()
print("Baseline:", baseline_metrics)

train_result = trainer.train()

finetuned_metrics = trainer.evaluate()
print("After fine-tuning:", finetuned_metrics)

metrics_keys = ["eval_accuracy", "eval_f1"]
labels = ["Accuracy", "F1"]

before = [baseline_metrics[k] for k in metrics_keys]
after  = [finetuned_metrics[k] for k in metrics_keys]

plt.figure(figsize=(6,4))
x = range(len(labels))
plt.bar([i-0.2 for i in x], before, width=0.4, label="Before")
plt.bar([i+0.2 for i in x], after,  width=0.4, label="After")
plt.xticks(x, labels)
plt.title(f"IMDb: Last-layer + Head Fine-tuning{TITLE_SUFFIX}")
plt.ylabel("Score")
plt.ylim(0,1)
plt.legend()
plt.show()

epochs = list(range(args.num_train_epochs + 1))  # e.g., [0,1,2,3]

before_line = [baseline_metrics["eval_accuracy"]] * len(epochs)
after_line  = [finetuned_metrics["eval_accuracy"]] * len(epochs)

plt.figure(figsize=(6,4))
plt.plot(epochs, before_line, marker="o", linestyle="--", label="Before")
plt.plot(epochs, after_line,  marker="o", linestyle="-",  label="After")
plt.title(f"IMDb: Accuracy Before vs After (Last-layer + Head){TITLE_SUFFIX}")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.ylim(0,1)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

def predict(texts):
    enc = tokenizer(texts, return_tensors="pt", truncation=True, padding=True).to(device)
    model.eval()
    with torch.no_grad():
        probs = model(**enc).logits.softmax(dim=-1)
    preds = probs.argmax(dim=-1).tolist()
    return [
        {"text": t, "neg_prob": float(p[0]), "pos_prob": float(p[1]), "label": "pos" if y==1 else "neg"}
        for t, p, y in zip(texts, probs.cpu(), preds)
    ]

predict([
    "This movie was absolutely wonderful. The performances were touching.",
    "Boring plot and terrible acting. I want my time back."
])

save_dir = "outputs/imdb_last_layer/model"
trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir)
print("Saved to:", save_dir)