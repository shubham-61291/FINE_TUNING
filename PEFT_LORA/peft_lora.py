# -*- coding: utf-8 -*-
"""PEFT_LORA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGGU7jO06d1ty0axl0qKe2_U49LyKMaw
"""

!pip -q install transformers datasets evaluate accelerate peft bitsandbytes

import torch, matplotlib.pyplot as plt, os
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, TrainingArguments, Trainer, set_seed
)
from datasets import load_dataset
import evaluate
from peft import get_peft_model, LoraConfig, TaskType

# Disable W&B logging
os.environ["WANDB_DISABLED"] = "true"

set_seed(42)

SMALL_RUN = True
TITLE_SUFFIX = " (SMALL RUN)" if SMALL_RUN else ""

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
if device == "cuda":
    print("GPU:", torch.cuda.get_device_name(0))

dataset = load_dataset("imdb")
if SMALL_RUN:
    dataset["train"] = dataset["train"].shuffle(seed=42).select(range(4000))
    dataset["test"]  = dataset["test"].shuffle(seed=42).select(range(2000))
dataset

model_name = "bert-base-uncased"  # LoRA works well on BERT/Roberta

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

def tok_fn(batch):
    return tokenizer(batch["text"], truncation=True)

tokenized = dataset.map(tok_fn, batched=True, remove_columns=["text"])
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

num_labels = 2
base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)

# LoRA configuration
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,   # sequence classification
    r=8,                          # rank
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["query", "value"]  # LoRA applied to attention q,v projections
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()   # sanity check

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }

args = TrainingArguments(
    output_dir="outputs/imdb_lora",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-4,       # higher LR works since only LoRA params train
    weight_decay=0.01,
    logging_steps=50,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

baseline_metrics = trainer.evaluate()
print("Baseline (LoRA adapters not trained):", baseline_metrics)

train_result = trainer.train()

finetuned_metrics = trainer.evaluate()
print("After LoRA fine-tuning:", finetuned_metrics)

# Bar plot
metrics_keys = ["eval_accuracy", "eval_f1"]
labels = ["Accuracy", "F1"]

before = [baseline_metrics[k] for k in metrics_keys]
after  = [finetuned_metrics[k] for k in metrics_keys]

plt.figure(figsize=(6,4))
x = range(len(labels))
plt.bar([i-0.2 for i in x], before, width=0.4, label="Before")
plt.bar([i+0.2 for i in x], after,  width=0.4, label="After")
plt.xticks(x, labels)
plt.title(f"IMDb: LoRA Fine-tuning{TITLE_SUFFIX}")
plt.ylabel("Score")
plt.ylim(0,1)
plt.legend()
plt.show()

# Line plot (flat, before vs after)
epochs = list(range(args.num_train_epochs + 1))
before_line = [baseline_metrics["eval_accuracy"]] * len(epochs)
after_line  = [finetuned_metrics["eval_accuracy"]] * len(epochs)

plt.figure(figsize=(6,4))
plt.plot(epochs, before_line, marker="o", linestyle="--", label="Before")
plt.plot(epochs, after_line,  marker="o", linestyle="-",  label="After")
plt.title(f"IMDb: Accuracy Before vs After (LoRA){TITLE_SUFFIX}")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.ylim(0,1)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

save_dir = "outputs/imdb_lora/model"
trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir)
print("Saved to:", save_dir)