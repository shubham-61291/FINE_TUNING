QLoRA Fine-tuning with PEFT (IMDb Sentiment Analysis)
This project demonstrates QLoRA (Quantized LoRA), a Parameter-Efficient Fine-Tuning (PEFT) method, applied on the IMDb movie review dataset.
QLoRA combines 4-bit quantization (via bitsandbytes) with LoRA adapters to drastically reduce memory while maintaining strong accuracy. This enables fine-tuning even large language models (LLMs) on a single GPU like Colab T4.
________________________________________
ğŸ“Œ Project Overview
â€¢	Model: bert-base-uncased
â€¢	Dataset: IMDb reviews â†’ sentiment classification (positive / negative)
â€¢	Method:
1.	Load model in 4-bit quantized mode using bitsandbytes.
2.	Freeze backbone parameters.
3.	Insert LoRA adapters into query (Wq) and value (Wv) matrices of each attention layer.
4.	Train only LoRA adapters + classifier head.
________________________________________
âš¡ Features
â€¢	Memory efficient: 4-bit quantization shrinks model size by ~75%.
â€¢	Parameter-efficient: Only ~1% of weights trained.
â€¢	Supports Colab T4 GPUs (through bf16, no fp16 gradient issues).
â€¢	Plots before vs after accuracy & F1.
â€¢	Saves fine-tuned model for reuse.
________________________________________
ğŸ› ï¸ Setup
pip install -U bitsandbytes
pip install -U transformers accelerate peft datasets evaluate
________________________________________
ğŸš€ Training Workflow
1. Baseline
â€¢	Model loaded in 4-bit, LoRA adapters untrained.
â€¢	Accuracy near random (~50%).
2. QLoRA Fine-tuning
â€¢	Train adapters (rank = 8, alpha = 16, dropout = 0.1).
â€¢	Backbone stays frozen.
â€¢	Training done in bf16 precision for stability.
3. Evaluation
â€¢	Accuracy & F1 improve significantly after fine-tuning.
________________________________________
## ğŸ“Š Results

### Bar Chart â€“ Accuracy & F1
![Bar Chart](bar_chart.png)

### Line Chart â€“ Accuracy Across Epochs
![Line Chart](line_chart.png)
________________________________________
ğŸ”® Inference Demo
After training, run quick predictions:
predict([
    "This movie was absolutely wonderful. The performances were touching.",
    "Boring plot and terrible acting. I want my time back."
])
Example Output:
[
  {"text": "...wonderful...", "neg_prob": 0.01, "pos_prob": 0.99, "label": "pos"},
  {"text": "...terrible...",  "neg_prob": 0.98, "pos_prob": 0.02, "label": "neg"}
]
________________________________________
ğŸ“‚ Project Structure
peft_qlora.py                       # training script (exported from Colab)
outputs/
   imdb_qlora/model/                # saved fine-tuned model + tokenizer
images/
   bar_chart.png                    # Before vs After (Accuracy & F1)
   line_chart.png                   # Accuracy Before vs After across epochs
________________________________________
ğŸ‘¤ Author
Shubham Singh
________________________________________
ğŸ“œ License
MIT License
