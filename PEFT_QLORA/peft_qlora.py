# -*- coding: utf-8 -*-
"""PEFT_QLORA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGGU7jO06d1ty0axl0qKe2_U49LyKMaw
"""

!pip -q install transformers datasets evaluate accelerate peft bitsandbytes
!pip install -U bitsandbytes
!pip install -U transformers accelerate peft

import torch, matplotlib.pyplot as plt, os
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    BitsAndBytesConfig, TrainingArguments, Trainer, set_seed
)
from datasets import load_dataset
import evaluate
from peft import get_peft_model, LoraConfig, TaskType

os.environ["WANDB_DISABLED"] = "true"
set_seed(42)

SMALL_RUN = True
TITLE_SUFFIX = " (SMALL RUN)" if SMALL_RUN else ""

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

dataset = load_dataset("imdb")
if SMALL_RUN:
    dataset["train"] = dataset["train"].shuffle(seed=42).select(range(4000))
    dataset["test"]  = dataset["test"].shuffle(seed=42).select(range(2000))

model_name = "bert-base-uncased"   # Could also try "roberta-base"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

def tok_fn(batch):
    return tokenizer(batch["text"], truncation=True)

tokenized = dataset.map(tok_fn, batched=True, remove_columns=["text"])

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",      # NormalFloat4 (better accuracy)
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    bnb_4bit_use_double_quant=True
)

base_model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
    quantization_config=bnb_config,
    device_map="auto"   # put on GPU if available
)

lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["query", "value"]
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }

args = TrainingArguments(
    output_dir="outputs/imdb_qlora",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-4,
    weight_decay=0.01,
    logging_steps=50,
    bf16=torch.cuda.is_available(),   # ‚úÖ use bf16
    fp16=False,                       # ‚ùå turn off fp16
    report_to="none"
)


trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

baseline_metrics = trainer.evaluate()
print("Baseline:", baseline_metrics)

train_result = trainer.train()

finetuned_metrics = trainer.evaluate()
print("After QLoRA fine-tuning:", finetuned_metrics)

# üìä Bar plot ‚Äì Before vs After (Accuracy & F1)
metrics_keys = ["eval_accuracy", "eval_f1"]
labels = ["Accuracy", "F1"]

before = [baseline_metrics[k] for k in metrics_keys]
after  = [finetuned_metrics[k] for k in metrics_keys]

plt.figure(figsize=(6,4))
x = range(len(labels))
plt.bar([i-0.2 for i in x], before, width=0.4, label="Before")
plt.bar([i+0.2 for i in x], after,  width=0.4, label="After")
plt.xticks(x, labels)
plt.title(f"IMDb: QLoRA Fine-tuning{TITLE_SUFFIX}")
plt.ylabel("Score")
plt.ylim(0,1)
plt.legend()
plt.show()

# üìà Line plot ‚Äì Before vs After Accuracy
epochs = list(range(args.num_train_epochs + 1))
before_line = [baseline_metrics["eval_accuracy"]] * len(epochs)
after_line  = [finetuned_metrics["eval_accuracy"]] * len(epochs)

plt.figure(figsize=(6,4))
plt.plot(epochs, before_line, marker="o", linestyle="--", label="Before")
plt.plot(epochs, after_line,  marker="o", linestyle="-",  label="After")
plt.title(f"IMDb: Accuracy Before vs After (QLoRA){TITLE_SUFFIX}")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.ylim(0,1)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

save_dir = "outputs/imdb_qlora/model"
trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir)
print("Saved to:", save_dir)