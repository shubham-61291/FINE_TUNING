# FINE_TUNING
A complete showcase of modern Transformer fine-tuning strategies â€” from full fine-tuning to parameter-efficient PEFT

Transformer Fine-Tuning Experiments (Head-only, Last-layer, LoRA, QLoRA)

This repository is a comprehensive showcase of fine-tuning strategies for Transformers, applied to the IMDb Sentiment Classification
 task.

It explores traditional fine-tuning (head-only, last-layer) alongside parameter-efficient fine-tuning (PEFT) methods (LoRA and QLoRA).

ğŸ“Œ Project Structure
FINE_TUNING/
â”‚
â”œâ”€â”€ NEW_HEAD/       # Head-only fine-tuning
â”œâ”€â”€ LAST_LAYER/     # Last-layer + head fine-tuning
â”œâ”€â”€ PEFT_LORA/      # LoRA fine-tuning (PEFT)
â”œâ”€â”€ PEFT_QLORA/     # QLoRA fine-tuning (PEFT + 4-bit quantization)
â”‚
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md       # (this file)


Each subfolder contains:

Training script (.py)

Results (bar chart + line chart)

README.md explaining the method + results

ğŸ”¹ Methods Implemented
1ï¸âƒ£ Head-only Fine-tuning (NEW_HEAD)

Freeze the pretrained model.

Train only a new classification head.

Fast, lightweight, but limited adaptation.

## ğŸ“Š Results

### Bar Chart â€“ Accuracy & F1
![Bar Chart](NEW_HEAD/BAR_CHART.png)

### Line Chart â€“ Accuracy Across Epochs
![Line Chart](NEW_HEAD/LINE_CHART.png)


2ï¸âƒ£ Last-layer Fine-tuning (LAST_LAYER)

Train the classification head and the last Transformer block.

Balances compute and performance.

## ğŸ“Š Results

### Bar Chart â€“ Accuracy & F1
![Bar Chart](LAST_LAYER/bar_chart.png)

### Line Chart â€“ Accuracy Across Epochs
![Line Chart](LAST_LAYER/Line_chart.png)


3ï¸âƒ£ LoRA Fine-tuning (PEFT_LORA)

Insert low-rank adapters into Query & Value projection matrices.

Train only adapters + head, freeze backbone.

Efficient, memory-friendly, strong performance.

## ğŸ“Š Results

### Bar Chart â€“ Accuracy & F1
![Bar Chart](PEFT_LORA/bar_chart.png)

### Line Chart â€“ Accuracy Across Epochs
![Line Chart](PEFT_LORA/line_chart.png)


4ï¸âƒ£ QLoRA Fine-tuning (PEFT_QLORA)

Load backbone in 4-bit NF4 quantized mode (bitsandbytes).

Add LoRA adapters for efficient fine-tuning.

Enables training larger models on a single GPU.

## ğŸ“Š Results

### Bar Chart â€“ Accuracy & F1
![Bar Chart](PEFT_QLORA/bar_chart.png)

### Line Chart â€“ Accuracy Across Epochs
![Line Chart](PEFT_QLORA/line_chart.png)


âš¡ Parameter Efficiency Comparison
Method	Trainable Params	GPU Memory	Accuracy Gain
Head-only	~1â€“2%	Low	âœ… Limited
Last-layer	~10â€“15%	Medium	âœ… Good
LoRA (PEFT)	~1%	Low	âœ… Strong
QLoRA (PEFT + 4b)	~1%	Ultra-Low	âœ… Strong
ğŸ› ï¸ Setup
pip install -U transformers datasets evaluate accelerate peft bitsandbytes

ğŸ‘¤ Author

Shubham Singh

ğŸ“œ License

MIT License
