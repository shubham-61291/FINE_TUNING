# -*- coding: utf-8 -*-
"""Fine_tunning_new_head_addiiton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGGU7jO06d1ty0axl0qKe2_U49LyKMaw
"""

!pip -q install transformers datasets evaluate accelerate

import torch, matplotlib.pyplot as plt
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, TrainingArguments, Trainer, set_seed
)
from datasets import load_dataset
import evaluate, os

# Disable W&B (prevents API key prompt)
os.environ["WANDB_DISABLED"] = "true"

set_seed(42)

# Run small subset for Colab speed
SMALL_RUN = True
TITLE_SUFFIX = " (SMALL RUN)" if SMALL_RUN else ""

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
if device == "cuda":
    print("GPU:", torch.cuda.get_device_name(0))

dataset = load_dataset("imdb")

if SMALL_RUN:  # shrink dataset for faster training
    dataset["train"] = dataset["train"].shuffle(seed=42).select(range(4000))
    dataset["test"]  = dataset["test"].shuffle(seed=42).select(range(2000))

dataset

model_name = "distilbert-base-uncased"   # small & Colab friendly
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

def tokenize_fn(batch):
    return tokenizer(batch["text"], truncation=True)

tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=["text"])
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
tokenized

num_labels = 2
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)

# Freeze entire backbone
for p in model.base_model.parameters():
    p.requires_grad = False

# âœ… New classification head (Linear layer) is automatically added by Hugging Face.
# Only this head will be trainable.
trainable = [n for n,p in model.named_parameters() if p.requires_grad]
print("Trainable parameters (should only be classifier head):", trainable)

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }

args = TrainingArguments(
    output_dir="outputs/imdb_head_only",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=5e-4,               # higher LR okay for head-only
    weight_decay=0.0,
    logging_steps=50,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

baseline_metrics = trainer.evaluate()
print("Baseline (random head, frozen backbone):", baseline_metrics)

train_result = trainer.train()

finetuned_metrics = trainer.evaluate()
print("Fine-tuned (head trained):", finetuned_metrics)

metrics_to_plot = ["eval_accuracy", "eval_f1"]
labels = ["Accuracy", "F1"]

before = [baseline_metrics[m] for m in metrics_to_plot]
after  = [finetuned_metrics[m] for m in metrics_to_plot]

plt.figure(figsize=(6,4))
x = range(len(labels))
plt.bar([i-0.2 for i in x], before, width=0.4, label="Before fine-tuning")
plt.bar([i+0.2 for i in x], after,  width=0.4, label="After fine-tuning")
plt.xticks(x, labels)
plt.title(f"IMDb Sentiment: Before vs After Fine-tuning{TITLE_SUFFIX}")
plt.ylabel("Score")
plt.ylim(0,1)
plt.legend()
plt.show()

epochs = list(range(args.num_train_epochs+1))  # e.g., [0,1,2,3]

before_line = [baseline_metrics["eval_accuracy"]]*len(epochs)
after_line  = [finetuned_metrics["eval_accuracy"]]*len(epochs)

plt.figure(figsize=(6,4))
plt.plot(epochs, before_line, marker="o", linestyle="--", color="red", label="Before fine-tuning")
plt.plot(epochs, after_line,  marker="o", linestyle="-",  color="green", label="After fine-tuning")

plt.title(f"IMDb Sentiment: Accuracy Before vs After{TITLE_SUFFIX}")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.ylim(0,1)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

def predict(texts):
    enc = tokenizer(texts, return_tensors="pt", truncation=True, padding=True).to(device)
    model.eval()
    with torch.no_grad():
        probs = model(**enc).logits.softmax(dim=-1)
    preds = probs.argmax(dim=-1).tolist()
    return [
        {"text": t, "neg_prob": float(p[0]), "pos_prob": float(p[1]), "label": "pos" if y==1 else "neg"}
        for t, p, y in zip(texts, probs.cpu(), preds)
    ]

predict([
    "This movie was absolutely wonderful. The performances were touching.",
    "Boring plot and terrible acting. I want my time back."
])

save_dir = "outputs/imdb_head_only/model"
trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir)
print("Saved model to:", save_dir)